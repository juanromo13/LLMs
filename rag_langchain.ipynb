{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e08c8c8-eb2d-4d56-a5c6-21e80fd1bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dotenv loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Dotenv loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf865d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "documents = ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need\n",
    "\n",
    "for doc in documents:\n",
    "    content = doc[0].page_content\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c4ec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative AI for Healthcare:\\nFundamentals, Challenges, and Perspectives\\nGang Chen1, Changshuo Liu3, Gene Anne Ooi4, Marcus Tan5, Zhongle Xie2,\\nJianwei Yin1,2, James Wei Luen Yip5, Wenqiao Zhang2, Jiaqi Zhu3* & Beng Chin Ooi1,2\\n1College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China\\n2College of Software Technology, Zhejiang University, Ningbo 315100, China\\n3School of Computing, National University of Singapore, Singapore 117417\\n4Singapore General Hospital, Singapore 169608\\n5National University Hospital, Singapore 119074\\nAbstract\\nGenerative Artificial Intelligence (GenAI) is taking the world by storm. It promises transformative opportunities for advanc-\\ning and disrupting existing practices, including healthcare. From large language models (LLMs) for clinical note synthesis\\nand conversational assistance to multimodal systems that integrate medical imaging, electronic health records (EHRs), and\\ngenomic data for decision support, GenAI is transforming the practice of medicine and the delivery of healthcare, such as\\ndiagnosis and personalized treatments, with great potential in reducing the cognitive burden on clinicians, thereby improving\\noverall healthcare delivery. However, GenAI deployment in healthcare requires an in-depth understanding of healthcare tasks\\nand what can and cannot be achieved.\\nIn this paper, we propose a data-centric paradigm in the design and deployment of GenAI systems for healthcare. Specif-\\nically, we reposition the data life cycle by making the medical data ecosystem as the foundational substrate for generative\\nhealthcare systems. This ecosystem is designed to sustainably support the integration, representation, and retrieval of di-\\nverse medical data and knowledge. With effective and efficient data processing pipelines, such as semantic vector search and\\ncontextual querying, it enables GenAI-powered operations for upstream model components and downstream clinical appli-\\ncations. Ultimately, it not only supplies foundation models with high-quality, multimodal data for large-scale pretraining\\nand domain-specific fine-tuning, but also serves as a knowledge retrieval backend to support task-specific inference via the\\nagentic layer. The ecosystem enables the deployment of GenAI for high-quality and effective healthcare delivery.\\nKeywords\\nGenerative AI, Healthcare, Foundation Model\\n1\\nIntroduction\\nThe rapid advancement of Generative Artificial Intelligence (GenAI) is creating unprecedented oppor-\\ntunities for healthcare. Large language models (LLMs) [1–4] are being applied to tasks such as clinical\\nnote synthesis and conversational assistance, while multimodal intelligence systems [5–11] that integrate\\nmedical imaging, electronic health records (EHRs), and genomic data are emerging as powerful tools\\nfor sophisticated decision support. Such developments highlight the potential of GenAI to accelerate\\ndiagnosis, enable personalized treatments, and alleviate the cognitive burden on clinicians.\\nWhile model innovation in GenAI keeps “tachytely” evolving, driven by advances in large-scale pre-\\ntraining, in-context learning, and architectural scaling [12, 13], sustained progress depends on a funda-\\nmental systemic transformation that treats data as coequal with models. As illustrated in Figure 1, the\\ntransformation entails a shift from static, model-centric pipelines to evolving, data-centric ecosystems.\\nWithin such ecosystems, the performance of foundation models depends heavily on the quality, quan-\\ntity, and diversity of their data [14–17], and relies on effective large-scale pretraining, domain-specific\\nfine-tuning, and retrieval augmented generation. The trajectory of the GPT series exemplifies this de-\\npendency, in which progressively larger and higher quality datasets, and effective prompt engineering,\\nhave resulted in better performance with relatively similar model architectures.\\nDespite the central role of data in enabling GenAI, the current medical data landscape remains highly\\nfragmented, heterogeneous, and siloed across institutions, modalities, and systems. Clinical data, includ-\\ning structured EHRs, free-text notes, medical images, physiological signals, and genomic information,\\nare often stored in disparate formats with inconsistent standards, limited interoperability, and restricted\\naccessibility [18, 19]. This not only hampers model training and evaluation but also limits the scalability\\n* Corresponding author (email: jiaqi77@nus.edu.sg)\\narXiv:2510.24551v1  [cs.AI]  28 Oct 2025\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n2\\nData Collection\\nModeling\\nHealthcare\\nApplications\\nModel Architecture\\nDesign\\nTraining Strategy\\nDesign\\nModel Training\\nModel Inference\\nModel Adaptation\\n...\\nData Preparation\\n(a)Static model-centric pipeline\\nModel\\nDesign\\nModel\\nTraining\\nModel\\nAdaptation\\nModel\\nInference\\nHealthcare\\nApplications\\nAcquisition\\nIngestion\\nData\\nFeedback\\nAnnotation\\nGovernance\\nData Lifecycle\\n(b)Evolving data-centric ecosystem for healthcare applications\\nFigure 1\\nEvolution to co-evolving healthcare GenAI with a sustainable data ecosystem.\\nand generalizability of GenAI applications across real-world settings. Furthermore, annotation bottle-\\nnecks, privacy regulations, and institutional policies severely constrain the availability of large-scale and\\nhigh-quality datasets [20]. Recent efforts [21–24] have introduced medical foundation models pretrained\\non domain-specific corpora (e.g., PubMed1), MIMIC2), or multi-hospital imaging archives), as summa-\\nrized in Figure\\n2. Unfortunately, many of existing models still struggle with incomplete supervision,\\ndata bias, and poor alignment with downstream tasks. Other emerging solutions, such as federated learn-\\ning [25, 26], synthetic data generation [27], and self-supervised representation learning [28, 29] may be\\nable to resolve some of the issues. Nevertheless, a robust, intelligent, and scalable data ecosystem for\\nGenAI in healthcare remains elusive.\\nAs opposed to viewing data as a passive, one-off input to model-centric GenAI healthcare pipelines, we\\nadvocate a sustainable data ecosystem that co-evolves with the foundation models and clinical applica-\\ntions to enable reliable, scalable, and context-aware generative capabilities. To realize this paradigm, we\\nidentify three interlocking challenges.\\nFirstly, the inherent fragmentation and heterogeneity of medical\\ndata present fundamental obstacles to effective utilization. Secondly, comprehensive data management\\nand governance frameworks remain largely aspirational rather than operational. Lastly, current healthcare\\nsystems lack the essential infrastructure for data–model co-evolution, limiting the continual refinement\\nand contextual adaptability. Overall, the translation of GenAI from proof-of-concept demonstrations\\nto practical clinical deployment depends fundamentally on realizing Data Value through three pillars:\\ncohesive data integration across modalities, rigorous life cycle governance frameworks, and sophisticated\\ninfrastructure supporting data–model co-evolution. Only through addressing these interconnected chal-\\nlenges can we bridge the gap between GenAI’s theoretical potential and its transformative application in\\nclinical practice.\\nTo explore this vision, we propose a data-centric GenAI system for healthcare that fundamentally\\nreconceptualizes the medical data life cycle. We position SAGE-Health, a Sustainable, Adaptive, and\\nGenerative Ecosystem for Healthcare, which serves as a comprehensive medical data ecosystem as the\\nfoundational substrate for generative healthcare systems, designed to sustainably integrate, represent,\\nand retrieve diverse medical data and knowledge sources. SAGE-Health enables GenAI-powered oper-\\nations through intelligent data management services, including semantic vector search and contextual\\nquerying capabilities. The architecture serves a dual purpose: supplying foundation models with high-\\nquality, multimodal data for large-scale pretraining and domain-specific fine-tuning, while simultaneously\\nfunctioning as a knowledge retrieval backend that supports task-specific inference via the agentic layer.\\nMoreover, the system incorporates a continuous improvement mechanism whereby user interactions and\\napplication-level feedback are systematically channeled back into the data ecosystem. This feedback loop\\nenables dynamic data refinement and ongoing system evolution, ensuring that data quality, model per-\\n1) https://pubmed.ncbi.nlm.nih.gov/\\n2) https://mimic.mit.edu/\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n3\\n2020 & Earlier\\n2021\\n2022\\n2023\\n2024 & Beyond\\nMultimodal\\nUnimodal\\nMedCLIP\\nMGCA\\nBioViL-T\\nMed-ST\\nHealthGPT\\nEyecareGPT\\nHeartcareGPT\\nHuatuoGPT-o1\\nHuatuoGPT\\nLLaVA-Med\\nMed-Flamingo \\nMedSAM\\nMedical SAM 2\\nCLIP, Flamingo\\nEmergence of Multimodal \\nFoundation Models\\nPre-Foundation Model Era\\nVilBERT, LXMERT\\nMultimodal GenAI for Healthcare\\nBioMistra\\nPMC-LLaMA\\nMed-PALM2\\nMediTron\\nBioGPT\\nGatorTron\\nClinicalBERT\\nPubMedBERT\\nBIOT\\nLaBraM\\nBENDR\\nSwin-UNet\\nUNETR\\nMedT\\n3D U-Net\\nAttention U-Net\\nFigure 2\\nEvolution of GenAI for healthcare: From the pre-foundation model era to domain-specific medical foundation models.\\nformance, and clinical utility continuously enhance one another over time. In summary, this integration\\nestablishes a self-reinforcing cycle that drives increasingly sophisticated and clinically relevant GenAI\\nsolutions for healthcare.\\nThe structure of this paper is organized as follows. Section 2 presents relevant background on the\\nfoundations of GenAI in healthcare. Section 3 reviews existing GenAI-powered applications across key\\nhealthcare scenarios. In Section 4, we identify critical challenges and delineate the key features central\\nto the generative healthcare systems. Section 5 outlines a conceptual roadmap for building GenAI-ready\\ndata ecosystems. We conclude the paper in Section 6 with a summary of insights and future directions.\\n2\\nFoundations of Generative AI in Healthcare\\nMany deep learning models have been proposed and deployed for healthcare, including healthcare model\\nzoos as the one in Apache SINGA [16]. They have been tailored for specific regression or prediction\\ntasks and are often based on specific datasets. Healthcare GenAI, making use of much larger amounts\\nof heterogeneous data, provides multimodal understanding and generation across a wide range of clinical\\nand biomedical scenarios. In this section, we shall briefly present the core building blocks of GenAI\\nfor healthcare, outlining the representative foundational models and infrastructures that underpin its\\ndevelopment. Figure 2 provides a partial list of healthcare foundation models, unimodal and multimodal,\\nproposed in recent years.\\n2.1\\nText-centric Foundation Models\\nWith the advent of Transformer-based architectures, Bidirectional Encoder Representations from Trans-\\nformers (BERT) [30] has driven the rapid evolution of domain-specific language models for clinical text\\nunderstanding. Early efforts, such as ClinicalBERT [21] and BEHRT [31], extend BERT-style pretraining\\nto EHRs and clinical notes, significantly enhancing capabilities in information extraction, risk prediction,\\nand event detection. PubMedBERT [22] further enhances domain semantic representation by pretraining\\ndirectly on biomedical literature from scratch, while GatorTron [23, 24] scales up both model size and\\ntraining corpus, pushing clinical NLP toward higher precision and broader coverage. For structured and\\ntemporal medical data, Med-BERT [32] targets longitudinal EHR sequences to enable dynamic health\\nmonitoring and risk stratification. These models leverage transformer [33] architectures to model complex\\ntemporal dependencies, thereby improving personalized medicine and clinical decision support.\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n4\\nBeyond BERT-style encoders, a new generation of large-scale, generative medical foundation models\\nhas emerged, shifting from bidirectional masked-language modeling to instruction-tuned, autoregres-\\nsive architectures. PMC-LLaMA [1] adapts the LLaMA framework through continual pretraining on\\nbiomedical literature and textbooks, achieving strong performance on medical question answering and\\nreasoning. Med-PaLM [2] and its successor Med-PaLM 2 [3] extend Google’s PaLM [34, 35] architec-\\nture with large-scale instruction tuning, retrieval-augmented reasoning, and ensemble refinement, achiev-\\ning state-of-the-art performance on MedQA [36] and the United States Medical Licensing Examination\\n(USMLE).\\n2.2\\nPhysiological Signal Foundation Models\\nPhysiological signal modeling has emerged as a critical component of generative AI in healthcare, driven\\nby the need to extract clinically meaningful patterns from complex sequential data such as electrocardio-\\ngrams (ECG) and electroencephalograms (EEG). Recent advances have been driven by transformer-based\\narchitectures and self-supervised pretraining, which enable scalable, task-agnostic feature learning from\\nlarge volumes of biosignals.\\nFor example, BENDR [37] combined masked autoencoding and contrastive\\nlearning to enhance EEG representations across tasks and paradigms, while EEG2VEC [38] jointly opti-\\nmized contrastive and reconstruction losses for robust local–global feature extraction, with the pretrained\\nmodel serving as a feature extractor for downstream tasks. Extending these ideas, ECGBERT [39] applied\\nLLM-style masked modeling to ECGs, uncovering latent language-like structures for diverse diagnostic\\ntasks with minimal fine-tuning. EEGPT [40] further advanced this paradigm as a pretrained transformer\\nmodel designed for universal EEG feature extraction, incorporating spatio-temporal representation align-\\nment and hierarchical processing to yield high-quality features for broad biomedical applications.\\nMoving toward greater generalizability, the Biosignal Transformer (BIOT) [41] introduced a flexible en-\\ncoder for cross-dataset learning, tokenizing heterogeneous biosignals (e.g., EEG, ECG, sensory data) into\\nfixed-length segments to improve robustness across variable formats. Similarly, the Large Brain Model\\n(LaBraM) [42] leveraged vector-quantized neural codes and masked code prediction for cross-dataset EEG\\npretraining, achieving state-of-the-art performance across diverse physiological signal modeling tasks.\\n2.3\\nMedical Imaging Foundation Models\\nIn the medical imaging domain, architectural innovations have driven the shift from task-specific solu-\\ntions to generalizable foundation models, reshaping how visual representations are learned and trans-\\nferred. U-Net [43] has played a foundational role in medical image analysis, serving both as a benchmark\\narchitecture and a building block for many foundation models.\\nOriginally introduced for biomedical\\nsegmentation, its encoder–decoder structure with skip connections enables precise localization from lim-\\nited annotated data, making it highly adaptable across imaging modalities. Building on this foundation,\\n3D U-Net [44] extends the framework to volumetric CT and MRI. Attention U-Net [45] incorporates\\nattention mechanisms to selectively emphasize salient regions. nnU-Net [46] automates architecture and\\nhyperparameter optimization, establishing a task-agnostic baseline for medical image segmentation.\\nWith the rise of Vision Transformers (ViTs) [47], researchers have begun exploring their advantages\\nover purely convolutional architectures, particularly their capacity for modeling long-range dependencies\\nand flexible contextual reasoning. Medical Transformer (MedT) [48] proposes gated position-sensitive\\naxial attention to improve training efficiency and contextual representation for medical images. Addi-\\ntionally, hybrid models such as UNETR [49] and Swin-UNet [50] integrate transformer encoders with\\nU-Net-style decoders to combine global contextual reasoning with high-resolution reconstruction. More\\nrecently, Segment Anything Model (SAM) [51, 52] has emerged as a foundation model for universal image\\nsegmentation, and its medical adaptations, such as MedSAM [53] and Medical SAM 2 [54], are being lever-\\naged as general-purpose building blocks for diverse downstream imaging tasks. This evolution illustrates a\\nclear trajectory from CNN-based architectures optimized for segmentation toward transformer-enhanced\\nhybrids that bridge local detail and global context, laying the groundwork for modern foundation models\\nin medical imaging.\\n2.4\\nMultimodal Foundation Models\\nIn recent years, foundational models in the medical domain have witnessed rapid development, with multi-\\nmodal models achieving significant progress in cross-modal data fusion and representation learning. Since\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n5\\nthe release of CLIP [55], which demonstrated the power of large-scale image–text contrastive pretraining,\\nmultimodal learning has emerged as a central research paradigm. Building on this trend, Flamingo [56]\\nintroduced a unified architecture for handling interleaved visual and textual inputs, advancing the design\\nof scalable and versatile multimodal systems.\\nTranslating these advances into the medical domain, MedCLIP [5] has emerged as a powerful tool for\\nmedical image–text alignment, substantially improving diagnostic assistance and electronic health record\\n(EHR) analysis, and laying the groundwork for deep exploitation of multimodal medical data. Multi-\\nGranularity Cross-modal Alignment (MGCA) framework [6] learns generalized medical visual represen-\\ntations by aligning medical images and radiology reports at pathological region-level, instance-levels, and\\ndisease-level through cross-attention and contrastive learning. BioViL [57] represents another key biomed-\\nical vision–language foundation model, improving image–report alignment through domain-adapted se-\\nmantic pretraining and enhanced cross-modal representation learning. To exploit the temporal struc-\\nture of medical data, such as descriptions of disease progression commonly found in clinical reports,\\nBioViL-T [7] extends this framework by incorporating temporal context from prior images and longitudi-\\nnal reports, enabling more accurate and context-aware performance across disease classification, phrase\\ngrounding, and report generation tasks. Medical Spatio-Temporal (Med-ST) [8] introduces a medical\\nspatio-temporal pretraining framework that jointly models the spatial relationships in multi-view chest\\nX-rays (e.g., posteroanterior and lateral views) and the temporal progression captured in patients’ histor-\\nical diagnostic records, thereby improving the capacity of vision–language models to leverage fine-grained\\ncross-view and longitudinal information for enhanced multimodal representation learning.\\n3\\nGenAI in Healthcare Applications\\nThe rapid evolution of foundation models has established GenAI as a cornerstone for advancing health-\\ncare applications [58]. Unlike traditional models optimized for narrow, task-specific objectives, GenAI\\nleverages large-scale pretraining, cross-modal representation learning, and generative reasoning to address\\ncomplex clinical needs. These capabilities make it particularly suitable for applications that require inte-\\ngrating heterogeneous biomedical data, generating context-aware outputs, and adapting to diverse clinical\\nscenarios. In practice, such strengths have driven progress across three pivotal domains: (i) Disease Di-\\nagnosis and Decision Support, where multimodal generative models unify imaging, physiological signals,\\nand clinical narratives to support decision-making; (ii) Medical Report Generation and Documentation,\\nwhere language models automate the synthesis of structured and unstructured records; and (iii) Drug\\nDiscovery and Biomedical Research, where molecular and protein language models accelerate hypothesis\\ngeneration and compound design. The following subsections review representative efforts in these areas,\\nanalyzing how generative and multimodal foundation models have been developed for modern healthcare\\napplications.\\n3.1\\nDisease Diagnosis and Decision Support\\nDisease diagnosis presents a uniquely complex challenge in healthcare, requiring the integration of diverse\\ninformation sources, ranging from imaging and physiological signals to clinical notes and longitudinal\\nhealth records. Recent advances in GenAI have driven a wave of diagnostic tools that move beyond\\ntraditional pattern recognition, enabling contextual reasoning and multimodal knowledge integration.\\nLLMs for Clinical Question Answering. Large Language Models (LLMs) have emerged as powerful\\ntools for clinical question answering (QA), leveraging instruction tuning, retrieval augmentation, and\\nchain-of-thought (CoT) reasoning to provide contextually grounded responses to complex medical queries.\\nBy integrating vast biomedical corpora and aligning with domain-specific instructions, these models\\nare capable of understanding nuanced clinical questions, synthesizing knowledge across specialties, and\\ngenerating evidence-backed answers.\\nThese methods can be broadly categorized into general-purpose medical QA models [1–4, 59, 60] and\\nspecialized medical QA models [61–64]. General-purpose models aim to provide broad coverage across\\ndiverse clinical topics, serving as versatile assistants for disease diagnosis and decision-making. BioMis-\\ntral [4] leverages open-source Mistral models [65] with biomedical continual pretraining and instruction\\nfine-tuning to support broad-coverage medical QA across general clinical topics. JMLR [60] employs a\\njointly trained retrieval-augmented generation (RAG) framework, integrating LLMs with retrieval mod-\\nules to access clinical guidelines and domain knowledge for medical QA. These models are designed\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n6\\nto operate as generalist assistants, capable of handling diverse medical queries across specialties to as-\\nsist disease diagnosis and clinical decision-making. Furthermore, specialized medical QA LLMs [61–64]\\nare being developed to address narrower clinical domains and patient populations. For instance, Pedi-\\natricsGPT [64] is a Chinese pediatric assistant LLM fine-tuned on pediatric guidelines and case-based\\ndialogues, providing tailored responses for pediatric care in Chinese clinical contexts. These develop-\\nments illustrate a continuum from broad-coverage generalists to focused domain experts, highlighting\\nhow strategic pretraining and fine-tuning align LLMs with the nuanced demands of clinical practice.\\nLVLMs for Diagnostic Reasoning.\\nThrough cross-modal pretraining and alignment, VLMs are\\ncapable of integrating biomedical imaging with domain-specific textual information, thereby providing\\ninterpretable, context-aware diagnostic insights.\\nFor example, LLaVA-Med [66] offers a cost-efficient\\nframework for training vision-language conversational assistants capable of answering open-ended research\\nquestions about biomedical images, supporting flexible diagnostic exploration.\\nIn addition, specialized VLMs have been developed for targeted diagnostic applications, with ra-\\ndiograph interpretation emerging as a major focus [9, 67–69].\\nKnowledge-enhanced Auto Diagnosis\\n(KAD) [69] integrates structured medical domain knowledge into vision–language pretraining, align-\\ning chest X-ray images with knowledge-grounded radiology report representations.\\nBy incorporating\\nontology-based concept encoding and disease-query mechanisms, KAD advances automated diagnosis for\\nchest X-ray images by leveraging domain knowledge. CARZero [9] replaces traditional cosine similarity-\\nbased alignment with cross-attention mechanisms and incorporates an LLM-driven prompt standard-\\nization strategy for zero-shot radiology classification, achieving state-of-the-art performance on chest\\nradiograph diagnostic benchmarks.\\nBeyond radiology, VLMs are also being tailored to other clinical\\nspecialties. For instance, OphGLM [70] extends this paradigm to ophthalmology, aligning ocular imag-\\ning with domain-specific knowledge for improved diagnostic reasoning in eye disease assessment, while\\nEchoCLIP [71] is an LVLM for echocardiography trained on over one million cardiac ultrasound videos\\nand expert interpretations, enabling accurate cardiac function assessment and patient-level reasoning\\nacross studies.\\nAgent-Based Diagnostic Assistants. Agent-based frameworks have recently emerged as a promising\\nparadigm for disease diagnosis and clinical decision support, addressing the limitations of standalone\\nLLMs by enabling multi-agent collaboration [72]. In these methods, agents are assigned specialized roles,\\nsuch as intent recognition, diagnostic reasoning, and treatment planning, to create a context-aware and\\ninteractive healthcare delivery process.\\nA growing body of frameworks underscores the potential of multi-agent systems in enhancing diagnos-\\ntic reasoning and clinical decision-making. The multi-agent system [73] simulates interactions between\\ndoctors and patients to refine diagnostic reasoning, demonstrating the effectiveness of multi-agent collab-\\noration in enhancing diagnostic performance. Similarly, Ke et al. [74] develop a multi-agent framework\\nthat assigns diverse roles to LLM-driven agents to mitigate cognitive biases in clinical decision-making,\\nsignificantly improving diagnostic accuracy in complex medical scenarios.\\nMedagent [75] proposes a\\nmulti-disciplinary role-playing framework in which LLM agents engage in multi-round collaborative dis-\\ncussions, enhancing medical reasoning and decision-making in zero-shot diagnostic scenarios.\\nMDa-\\ngents [76] introduces an adaptive collaboration framework that dynamically assigns solo or group roles\\nto LLM agents based on task complexity. Moreover, Agent Hospital [77] simulates the full clinical work-\\nflow using evolvable LLM-powered agents, with its MedAgent-Zero paradigm enabling doctor agents to\\niteratively improve through simulated and real-world experience, demonstrating strong generalization to\\nreal-world diagnostic tasks.\\nBeyond general diagnostic assistants, several frameworks target specialized clinical domains, demon-\\nstrating the adaptability of multi-agent systems to specific healthcare tasks. ClinicalAgent [78] integrates\\nlarge-scale domain knowledge with multi-agent collaboration to enhance clinical trial outcome prediction.\\nPolaris [79] focuses on real-time patient–AI clinical dialogues, using a safety-oriented constellation of pri-\\nmary and specialist agents trained on clinical care plans, regulatory documents, and simulated dialogues.\\nThese agent-based systems represent a shift from monolithic LLMs to dynamic, role-oriented multi-agent\\nsystems, enhancing reasoning and embedding domain expertise within clinical workflows.\\n3.2\\nMedical Report Generation and Documentation\\nMedical report is a cornerstone of clinical care but remains a cognitively and time-consuming demanding\\ntask for healthcare providers. GenAI offers the potential to streamline documentation by automating the\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n7\\nsynthesis of structured and unstructured information into coherent, contextually relevant reports, thereby\\nreducing administrative burden and enhancing communication among clinicians and with patients. Unlike\\nmodels designed for disease diagnosis, which focus on classification or decision support, report generation\\nmodels prioritize producing detailed content with specialized medical terminology that documents clinical\\nfindings [80].\\nVision–Language Encoder–Decoder Pipelines. Early approaches to report generation primarily\\nleverage encoder–decoder architectures that combine a visual feature extractor with a text generation\\nmodule. In this paradigm, a vision encoder (e.g., ResNet [81, 82], ViT [83, 84]) is used to transform\\nmedical images such as X-rays, CT, and pathology into high-dimensional feature representations. These\\nrepresentations are then passed to a language decoder that generates descriptive reports in natural lan-\\nguage. Several notable studies exemplify this paradigm. R2Gen [85] adopts a memory-driven Transformer\\nthat allows the decoder to revisit visual features, improving consistency and coverage in generated re-\\nports. Zhang et al. [86] extend the encoder–decoder pipeline by incorporating a graph convolutional\\nmodule over disease findings, enabling explicit modeling of relationships between abnormalities (e.g.,\\neffusion and cardiomegaly) and yielding more clinically accurate reports. These methods established\\nthe foundational blueprint for report generation, while their limitations in factual accuracy and clinical\\ncoverage motivated the shift toward more advanced multimodal and agent-based approaches.\\nEnd-to-End Multimodal Large Language Models. Unlike earlier encoder–decoder pipelines, recent\\nefforts pursue end-to-end multimodal large language models (MLLMs) that unify visual and textual\\nrepresentations within a single framework.\\nThese models aim to achieve tighter integration between\\nmedical image understanding and language generation, thereby reducing reliance on handcrafted fusion\\nstrategies and enabling more flexible downstream applications.\\nFor instance, XrayGPT [67] bridges\\nthe gap between general-purpose VLMs and radiology by aligning a medical visual encoder (MedClip)\\nwith a fine-tuned LLM (Vicuna) and introducing 217k interactive summaries from free-text radiology\\nreports, thereby equipping the model with strong visual–textual grounding. MRM [87] unifies radiograph\\nunderstanding and report comprehension through masked record modeling, jointly reconstructing image\\npatches and report tokens to learn transferable, knowledge-enhanced vision–language representations. To\\nfurther improve clinical accuracy in report generation, researchers have incorporated diverse strategies\\nsuch as adaptive patch–word alignment for improved explainability [88], training paradigms that promote\\ndeeper incorporation of visual features into LLMs [89], and multi-view longitudinal learning with patient-\\nspecific priors [80]. These innovations underscore the trend toward end-to-end multimodal reasoning,\\nadvancing both visual–textual alignment and the clinical reliability of generated reports.\\nAgent-based Medical Report Generation. More recently, agent-based methods represent a rapidly\\nemerging paradigm in the medical domain, emphasizing dynamic reasoning, planning, and interactive\\nworkflows with external tools and knowledge bases. In the context of radiology and broader medical\\nreporting, agent-based systems can function as autonomous assistants that query structured resources\\n(e.g., UMLS, RadLex, or EHR data), validate generated findings against medical guidelines, and even\\ncoordinate multi-step reasoning pipelines. Multimodal Multi-Agent RRG [90] adopts a modular design\\nthat mirrors the clinical reasoning workflow, deploying specialized agents for retrieval, drafting, visual\\nanalysis, refinement, and synthesis. RadCouncil [91] follows a council-style architecture, where agents\\nsequentially retrieve similar cases, generate impressions, and review outputs, ensuring that final reports\\nachieve higher clarity and diagnostic reliability. To enhance explainability, CBM-RAG [92] integrates\\ninterpretable concept bottleneck models with a multi-agent retrieval-augmented generation process, en-\\nabling reports that are grounded in explicit clinical evidence. MRGAgents [93] introduces disease-specific\\nagents trained on category-focused subsets of data, effectively mitigating the tendency of models to over-\\nreport normal findings and improving the comprehensiveness of abnormality detection. Although still at\\nan early stage, agent-based methods highlight a promising direction, integrating the reasoning flexibility\\nof autonomous agents with the domain expertise of medical knowledge bases to achieve more reliable,\\ntransparent, and clinically useful report generation.\\n3.3\\nDrug Discovery and Biomedical Research\\nDrug discovery is a complex, multi-stage pipeline that spans from target identification and validation to\\nlead compound generation, optimization, and clinical translation. Traditional approaches often require\\nmore than a decade of effort and substantial financial investment, with high attrition rates during clinical\\ntrials. Recent GenAI approaches are reshaping this paradigm, ranging from task-specific methods for\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n8\\nindividual pipeline stages to cross-stage agent-based frameworks that enable end-to-end workflows.\\nTask-Specific Drug Discovery Methods. A growing body of work leverages LLMs to enhance per-\\nformance in specific drug discovery subtasks.\\nFor example, LMFL [94] with logical feedback refines\\nmolecular generation through constraint-guided reasoning, improving the quality of novel pharmacolog-\\nical leads. DrugAgent [95] formulates drug–target interaction prediction as an agent-driven task, where\\nan LLM planner collaborates with domain-specific tools to iteratively search for high-performing models.\\nDrugAssist [96] introduces an interactive optimization model that employs human–AI dialogue to refine\\nmolecular properties across multiple objectives, thereby advancing molecule optimization. CLADD [97]\\nfurther extends this paradigm by adopting a multi-agent RAG framework that integrates biomedical\\nknowledge graphs for context-aware molecule evaluation.\\nCross-Stage and End-to-End Agent Frameworks. Beyond single-task improvements, recent efforts\\nemphasize building generalizable orchestration systems that span multiple pipeline stages. DrugPilot [98]\\nrepresents such an approach by introducing a parameterized memory pool to manage heterogeneous\\nbiomedical data, enabling multi-stage reasoning and efficient tool coordination for generation, optimiza-\\ntion, and property prediction.\\nSimilarly, Solovev et al. [99] decompose complex discovery tasks into\\nmanageable subtasks, with planner, validator, and tool-calling agents coordinating pretrained generative\\nmodels, docking algorithms, and predictive modules. These systems achieve near end-to-end automation,\\ndemonstrating superior performance in complex real-world drug discovery scenarios, such as neurodegen-\\nerative disease research.\\n3.4\\nMultifunctional Generalist Models\\nBeyond task-specific architectures, a new class of generalist foundation models has emerged, aiming to\\nunify diverse biomedical modalities and tasks within a single framework. These multifunctional systems\\nare designed to handle a wide spectrum of applications, including open-QA, closed-QA, and report or im-\\nage generation, by jointly leveraging domain-specific medical knowledge and multimodal reasoning capa-\\nbilities. BiomedGPT [100] represents a transformative approach that connects biological processes with\\nhuman communication, facilitating enhanced comprehension of essential biological mechanisms within\\nmedical contexts. HuatuoGPT [101] introduces PubMedVision, a collection of 1.3 million high-quality\\nmedical specimens that substantially enhances flexibility and performance across medical vision–language\\ntasks. Similarly, HealthGPT [10] presents a unified framework for medical vision-language comprehen-\\nsion and synthesis, employing a heterogeneous knowledge adaptation methodology that yields consistent\\nperformance gains across diverse applications.\\nExpanding to specialized diagnostic domains, EyecareGPT [11] incorporates an adaptive resolution sys-\\ntem for dynamic optimization alongside a layer-wise dense connector that effectively integrates multi-scale\\nvisual characteristics. These innovations reveal considerable promise for advancing open research initia-\\ntives in automated ophthalmological diagnostics. HeartcareGPT [102] develops an extensive multimodal\\nfoundation architecture featuring an innovative bidirectional ECG abstract tokenization methodology, en-\\nabling detailed electrocardiogram analysis and interpretation capabilities. Overall, these models illustrate\\nthe evolution of intelligent healthcare from single-modality to multimodal paradigms, from single-model\\nto multi-agent frameworks, and from single-task functionality to truly multifunctional generalist systems.\\n4\\nOn the Design of a Data-Centric Ecosystem\\nIn this section, we discuss the current challenges and delineate the key features required for a data-centric\\necosystem to support GenAI for healthcare.\\n4.1\\nChallenges\\nRecent years have witnessed an unprecedented growth in both the volume and heterogeneity of healthcare\\ndata, spanning structured EHRs, unstructured medical imaging, physiological waveforms, and emerging\\nomics profiles. Coupled with advances in GenAI, these rich multimodal resources hold transformative\\npotential for clinical decision support, medical documentation, patient engagement, and biomedical dis-\\ncovery.\\nWhile these approaches demonstrate significant potential, most remain confined to research\\nprototypes and struggle to achieve real-world deployment. For example, a model that generates echocar-\\ndiography summaries from imaging and waveform data may perform well on a local research dataset,\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n9\\nbut fail in deployment due to incompatible data formats, lack of reliable provenance tracking, or the\\nabsence of feedback loops to adapt to evolving clinical protocols. Through a systematic analysis of ex-\\nisting efforts across diverse healthcare applications, we identify three interlocking challenges rooted in\\nthe data ecosystem, spanning static data fragmentation, inadequate dynamic management, and the lack\\nof infrastructures for data–model co-evolution. These structural barriers form a critical bottleneck in\\ntranslating GenAI from proof-of-concept to safe, accurate, and broadly accessible clinical practice.\\n4.1.1\\nData Fragmentation and Heterogeneity\\nHealthcare data is inherently fragmented and heterogeneous, spanning structured EHR tables with lon-\\ngitudinal lab values and coded diagnoses; unstructured clinical narratives in discharge summaries; high-\\ndimensional imaging modalities such as MRI, CT, and histopathology slides; continuous physiological\\nwaveforms from ICU monitors; and emerging omics datasets from genomics and proteomics. Beyond\\nraw data, derived resources, such as semantic embeddings for retrieval-augmented generation (RAG) and\\ndomain-specific medical knowledge graphs, further diversify the ecosystem. These assets are often stored\\nin incompatible formats across institutions and governed by inconsistent metadata schemas, with varying\\ndegrees of data completeness, quality, and annotation fidelity depending on institutional standards and\\ndevice calibration. Multi-institutional datasets therefore exhibit uneven data quality and contribution\\nbalance, as some organizations supply larger or cleaner subsets while others contribute sparse or noisy\\nsamples. Such fragmentation appears not only as multi-format inconsistency but also as semantic, struc-\\ntural, and temporal discontinuities that disrupt knowledge alignment and longitudinal reasoning across\\nmodalities.\\nFor GenAI systems, these discontinuities propagate along the entire pipeline.\\nAt the input level,\\nfragmented data reduces contextual completeness. For instance, a RAG module may retrieve laboratory\\nresults without corresponding imaging evidence, yielding incomplete prompts.\\nAt the representation\\nlevel, modality-specific preprocessing and inconsistent annotations distort the embedding space, hindering\\ncross-modal alignment and leading to biased generation. At the generation level, incoherent metadata or\\nmissing temporal links cause factual inconsistencies in generated clinical summaries, such as reporting\\na treatment before its documented prescription. Ultimately, data fragmentation undermines GenAI’s\\nability to form a unified patient-centric understanding, erodes factual faithfulness, and limits model\\ntransferability across clinical sites.\\n4.1.2\\nData Lifecycle Management and Governance\\nEffective GenAI in healthcare relies not only on data availability but also on the continuous manage-\\nment of data across its entire lifecycle, from acquisition and curation to utilization, monitoring, and\\narchival. In practice, most institutions adopt static, one-off pipelines where data are ingested, cleaned,\\nand frozen for model training, without systematic procedures for versioning, quality auditing, or periodic\\nrenewal. Updates to clinical protocols, device calibration standards, or labeling criteria are seldom prop-\\nagated to existing datasets, resulting in temporal inconsistencies and concept drift across data vintages.\\nRecent consensus efforts in diagnostic AI [103] underscore the importance of documenting data acquisi-\\ntion protocols, preprocessing steps, and dataset partitioning to ensure transparency and reproducibility.\\nThese principles illustrate how inadequate lifecycle management undermines traceability and regulatory\\nassurance.\\nDeficiencies in data governance further amplify these lifecycle challenges. Inconsistent anonymization\\npractices and heterogeneous access-control policies hinder secure data sharing across institutions, while\\nthe absence of unified provenance tracking impedes accountability and interpretability. Prior analyses of\\nclinical AI governance frameworks have highlighted similar gaps in transparency and traceability [58, 104].\\nWithout continuous stewardship and harmonized oversight, generative systems rely on outdated, weakly\\ncontextualized information, compromising both factual reliability and compliance with evolving ethical\\nand regulatory standards.\\n4.1.3\\nData–Model Co-Evolution Infrastructure\\nCurrent GenAI pipelines in healthcare remain largely unidirectional: data flow into model training, yet\\nlittle information flows back to refine the underlying datasets. Consequently, models drift away from the\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n10\\nrealities they were designed to represent, while data pipelines remain passive recipients rather than ac-\\ntive participants in continuous improvement. Empirical assessments of large hospital data networks, such\\nas MIMIC-III [105] and the eICU Collaborative Research Database [106], reveal that record revisions,\\nlate entries, and retrospective corrections are propagated only partially or after prolonged delays (often\\nexceeding several months), illustrating how the absence of responsive infrastructure prevents timely syn-\\nchronization between data and model updates [107]. Ideally, this feedback loop would enable models\\nto surface systematic errors or underrepresented patterns, prompting targeted data enrichment or fine-\\ngrained re-training. For instance, a radiology report generation model that identifies misinterpretations\\nof rare fracture types could initiate both dataset augmentation for such cases and architectural refine-\\nment for improved spatial reasoning. Without such adaptive co-evolution, GenAI performance decays\\nover time, contextual relevance erodes, and clinical adoption stalls.\\n4.2\\nKey Features\\nBeyond addressing present challenges, a data-centric ecosystem for generative healthcare AI should be\\nenvisioned as a proactive, evolving foundation that treats data as a dynamic substrate for adaptive, trust-\\nworthy, and clinically aligned intelligence. We distill this perspective into four interdependent features:\\n• Organized Multimodal Representation and Interoperability. At the core of the ecosystem\\nlies the ability to reconcile the profound heterogeneity of healthcare data. Clinical knowledge is dis-\\ntributed across diverse modalities, from structured EHR tables and radiological images to continuous\\nphysiological signals and emerging omics profiles, each encoded in fundamentally different formats and\\nsemantics. To transform these fragmented resources into a coherent whole, data are systematically\\norganized under standardized schemas, then embedded into shared semantic representation spaces. Or-\\nganized in this manner, multimodal data can be consistently indexed, queried, and linked, establishing\\na coherent substrate on which higher-level reasoning and model adaptation can be built.\\n• Semantic Enrichment and Contextual Retrieval. Building on organized, interoperable data,\\nthe ecosystem derives semantically enriched assets that expose latent clinical relationships and tem-\\nporal dependencies. Raw records are transformed into higher-order abstractions such as embedding\\nvectors and temporal knowledge graphs, which capture latent associations across patients, diseases,\\nand modalities. These enriched resources underpin contextual retrieval, enabling generative models\\nto access task-relevant evidence dynamically during inference. By grounding outputs in semantically\\ncurated and clinically aligned information, enrichment and retrieval mechanisms support RAG, cross-\\nmodal diagnostic reasoning, and personalized decision support.\\n• Adaptive Feedback and Co-Evolution. A defining feature of a sustainable ecosystem is the capac-\\nity to learn continuously from its own use. Feedback signals arising from clinician interactions, model\\nperformance monitoring, and drift detection are systematically reintegrated into both data pipelines\\nand model refinement through concrete mechanisms such as re-indexing, relabel audits, and PEFT\\nrefresh. This feedback loop transforms the relationship between data and models from a one-way de-\\npendency into a bidirectional process of co-evolution: enriched datasets enhancing model generalization\\nwhile model-driven insights guide targeted data curation.\\n• Trustworthy and Federated Governance. Trust is embedded as an infrastructural property of\\nthe ecosystem through privacy-preserving computation, provenance tracking, and continuous regula-\\ntory compliance. Transparent auditability ensures confidence in data use and model outputs, while\\nfederated infrastructures, such as our own FALCON [26], enable multi-institutional collaboration with-\\nout centralizing sensitive records. In combining trustworthiness with federated design, the ecosystem\\nbalances large-scale knowledge sharing with local custodianship, supporting sustainable adoption in\\nclinical practice.\\n5\\nPerspectives\\n5.1\\nDesign of SAGE-Health\\nTo bridge the persistent gap between GenAI prototypes and real-world clinical workflows, we propose\\nSAGE-Health, a Sustainable, Adaptive, and Generative Ecosystem for Healthcare.\\nThe conceptual\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n11\\n①\\xa0Sustainable Medical Data Ecosystem\\xa0\\nModel Adaptation Hub\\nPrivacy-preserving\\nIntelligence\\n② Adaptive Medical GenAI\\xa0Layer\\n④ Healthcare\\xa0\\nApplication Layer\\nDisease Diagnosis &\\nDecision Support\\nMedical Report\\nGeneration\\n&\\xa0Documentation\\nDrug Discovery &\\nBiomedical Research\\nTask Planner & Decomposer\\nAgent Coordination Hub\\nCore Orchestration\\n...\\nPrompting\\nFinetuning ...\\n③ Agentic Collaboration Layer\\nDistributed Infrastructure / Federated Learning\\nIntelligent Data Management\\xa0and Governance\\nMedical Data Lakehouse\\nRaw Multimodal Data\\n...\\nKnowledge\\xa0\\nGraphs\\nCurated Semantic Data\\nVector DB\\n...\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0Expert Agent Suite\\nVector Search Engine\\nData Query Engine\\nNeurStore\\n\\xa0 \\xa0 Foundation Model Zoo\\nFigure 3\\nSAGE-Health Architecture.\\nframework of SAGE-Health is illustrated in Figure 3. SAGE-Health reconceptualizes the role of data\\nfrom a static, peripheral resource to a continuously evolving, intelligent substrate that co-develops in\\nsynergy with generative models.\\nThe framework integrates three core components: (1) Sustainable\\nMedical Data Ecosystem, (2) Adaptive Medical GenAI Layer, and (3) Agentic Collaboration Layer. These\\nthree components operate in concert to underpin the (4) Healthcare Application Layer, enabling scalable\\ndownstream tasks such as disease diagnosis, medical report generation, and drug discovery. By doing so,\\nSAGE-Health establishes a data-centric pathway for translating GenAI from controlled demonstrations\\nto safe, effective, and widely adopted clinical practice.\\n5.2\\nSustainable Medical Data Ecosystem\\nThe sustainable medical data ecosystem forms the foundational layer of SAGE-Health, designed to re-\\nstructure fragmented medical information into a unified and governable resource.\\nIt functions as an\\nevolving infrastructure that curates, manages, and governs healthcare data to sustain GenAI’s reliability\\nand clinical relevance over time. We categorize heterogeneous medical data into two interdependent tiers,\\nraw multimodal data (e.g., EHRs, imaging, and physiological signals) and curated semantic data (e.g.,\\nembedding vectors and medical knowledge graphs) derived from it. To accommodate both, we propose a\\ntwo-tier Medical Data Lakehouse architecture that preserves the full fidelity of original clinical records\\nwhile enriching them into machine-interpretable formats. Built on top of this foundation is Intelligent\\nData Management and Governance, whose core consists of a data query engine for structured and\\nunstructured access to the aforementioned raw data tier, and a vector search engine for high-dimensional\\nretrieval across multimodal embeddings within the semantic data tier. These engines operate in concert\\nto unify multimodal ingestion, semantic enrichment, and cross-modal retrieval. In healthcare contexts,\\ngovernance is critical to ensuring continuous privacy compliance with regulations such as HIPAA3) and\\nGDPR4), along with provenance tracking that links every model output to its originating data, process-\\ning steps, and consent records, maintaining a verifiable chain of custody from data acquisition to model\\noutput. Such measures not only safeguard patient confidentiality but also build trust in GenAI outputs,\\nenabling their safe and sustained integration into clinical workflows.\\nThe Sustainable Medical Data Ecosystem functions as the living backbone of SAGE-Health, continu-\\nously ingesting, harmonizing, and semantically enriching diverse healthcare data streams. Its operation\\nis interwoven with the other architectural layers, enabling both data provision and feedback-driven re-\\nfinement. With the Adaptive Medical GenAI Layer, the ecosystem delivers high-quality, context-aware\\ninputs for model training, prompting, and fine-tuning. In return, it receives model-driven feedback (e.g.,\\n3) Health Insurance Portability and Accountability Act, Available at: https://www.hhs.gov/hipaa\\n4) General Data Protection Regulation, Available at: https://gdpr-info.eu\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n12\\nerror patterns, uncertainty signals, drift detection) to guide targeted data enrichment and pipeline ad-\\njustments. In the Agentic Collaboration Layer, the ecosystem underpins retrieval-augmented generation\\n(RAG) workflows by providing agents with integrated multimodal retrieval capabilities. Agents can query\\nacross structured, unstructured, and high-dimensional semantic representations, ensuring that decision-\\nmaking is consistently grounded in comprehensive, up-to-date evidence.\\nThe Healthcare Application\\nLayer closes the feedback loop by indirectly contributing enriched real-world signals back into the data\\necosystem. Clinical applications not only consume model outputs but also produce high-value feedback in\\nthe form of user interactions, clinician corrections, and task outcome records. These signals flow through\\nthe Agentic Collaboration Layer, where they are contextualized with task metadata and provenance de-\\ntails before re-entering the data ecosystem. Here, the Intelligent Data Management and Governance com-\\nponent performs quality control (e.g., cleaning, labeling), compliance verification, semantic integration,\\nand re-insertion into the medical data lakehouse. Through these coordinated interactions, the Sustainable\\nMedical Data Ecosystem operates not merely as a data provider but as a strategic orchestrator, governing\\nthe flow, integrity, and contextual richness of information across SAGE-Health. This sustainable design\\ntransforms data from a static asset into a continuously improving, governance-compliant foundation for\\nadaptive and clinically aligned GenAI.\\n5.3\\nAdaptive Medical GenAI Layer\\nThe Adaptive Medical GenAI Layer forms the intelligence core of SAGE-Health, enabling generative\\nmodels to continuously learn, adapt, and operate in alignment with evolving clinical contexts. Its pri-\\nmary role is to leverage the high-quality, context-aware data curated by the Sustainable Medical Data\\nEcosystem to build and adapt models for a wide spectrum of specialized healthcare tasks, ensuring that\\ngenerative intelligence remains both clinically relevant and task-specific.\\nAt its foundation lies the Foundation Model Zoo, a curated collection of large-scale pre-trained\\nmodels spanning multiple modalities, such as LLMs for text and MLLMs for imaging, signals. These\\nmodels serve as the backbone for downstream adaptation, enabling rapid deployment across varied clinical\\ntasks without training from scratch. Built atop this foundation is the Model Adaptation Hub, which\\nprovides a flexible toolkit for tailoring models to specific healthcare scenarios. This includes zero- and\\nfew-shot prompt engineering for rapid task generalization, parameter-efficient fine-tuning (PEFT) such\\nas LoRA [108] and extra adapters for domain-specific optimization, and full supervised fine-tuning (SFT)\\nand architecture-level modifications when deeper specialization is required. The hub also integrates real-\\nworld feedback from the application layer, allowing continuous refinement through incremental learning\\nand drift-aware re-training strategies.\\nComplementing these adaptation capabilities is the Privacy-\\nPreserving Intelligence subsystem, which embeds privacy-by-design mechanisms into all stages of\\nmodel lifecycle management. Techniques such as differential privacy, zero-knowledge proof (ZKP) [109],\\nand secure multi-party computation (SMPC) ensure that sensitive patient data remains decentralized\\nwhile still contributing to collective model improvement, enabling secure model adaptation and inference\\nin compliance with stringent healthcare regulations.\\nOperationally, the Adaptive Medical GenAI Layer interacts closely with the Agentic Collaboration\\nLayer by exposing adaptable model endpoints that agents can query, orchestrate, and chain within com-\\nplex inference workflows. Through this interface, agents dynamically select optimal models and inference\\nstrategies, enabling the delivery of accurate, context-aware outputs to a wide range of downstream tasks\\nin the Healthcare Application Layer. Crucially, application-level user interactions, clinical corrections,\\nand performance metrics are routed back through the agentic layer, enriched with provenance and task\\nmetadata, and used to trigger incremental updates or re-training for model refinement. In this way, the\\nAdaptive Medical GenAI Layer not only serves as the execution engine for healthcare intelligence but\\nalso as a self-optimizing module, aligning model behavior with evolving clinical practices and application\\nrequirements.\\n5.4\\nAgentic Collaboration Layer\\nThe Agentic Collaboration Layer serves as the cognitive coordination hub of the SAGE-Health archi-\\ntecture, bridging adaptive generative intelligence with real-world healthcare applications. It transforms\\nraw model capabilities into task-oriented, context-aware actions through dynamic orchestration of spe-\\ncialized agents, as illustrated in Figure 4. By decomposing complex clinical objectives into executable\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n13\\nTask Planner & Decomposer\\nAgent Coordination Hub\\nCore Orchestration\\nExpert Agent Suite\\nTask-oriented\\xa0Agents\\nModel-oriented\\xa0Agents\\nData-oriented\\xa0Agents\\nGovernance\\xa0Agents\\n\\xa0 \\xa0 \\xa0 \\xa0Foundation Model Zoo\\n\\xa0 \\xa0 \\xa0 \\xa0 Medical Data Lakehouse\\nHealthcare\\xa0Applications\\nTask-oriented\\xa0Agents\\nModel-oriented\\xa0Agents\\nData-oriented\\xa0Agents\\nGovernance\\xa0Agents\\nData Retrieval (RAG)\\nData Feedback\\nCoordination\\nTask Interpretation\\nDecomposition &\\nAssignment\\nResult Integration\\nModel Selection\\nModel Adaptation\\nPolicy Enforcement\\nPrivacy & Compliance\\nAuditing\\nRisk & Safety Oversight\\nTask Lifecycle Progression\\n③ Agentic Collaboration Layer\\nFigure 4\\nOverview of the Agentic Collaboration Layer.\\nsubtasks, assigning them to the appropriate agents, and integrating results into coherent solutions, this\\nlayer enables flexible and auditable decision-making.\\nSpecifically, the Core Orchestration component governs this layer, with a Task Planner and De-\\ncomposer that interprets high-level clinical objectives and decomposes them into well-defined subtasks\\nwith explicit dependencies, and an Agent Coordination Hub that dynamically allocates these subtasks to\\noptimal agents in the Expert Agent Suite, ensuring seamless cooperation among diverse agent types.\\nIn particular, the Expert Agent Suite comprises four specialized agent classes, each designed to address\\ndistinct operational dimensions.\\nFirst, the Task-oriented Agents interpret domain-specific objectives,\\ndecompose workflows into executable sequences, and integrate partial results into coherent final outputs.\\nThey act as the bridge between abstract clinical intent and concrete multi-step execution plans. Second,\\nthe Model-oriented Agents manage the end-to-end lifecycle of GenAI model utilization. They select the\\nmost suitable model from the Foundation Model Zoo and perform model adaptation using techniques\\nfrom the Model Adaptation Hub to meet evolving healthcare needs. Third, the Data-oriented Agents\\nhandle the operations with the Sustainable Medical Data Ecosystem, performing RAG from the Medical\\nData Lakehouse to provide context-rich inputs for downstream tasks and coordinating feedback integra-\\ntion to enhance data quality and semantic consistency. Additionally, the Governance Agents safeguard\\nthe safety boundaries of the system throughout the entire task lifecycle progression. These agents se-\\nquentially enforce policies, audit privacy and regulatory compliance, and oversee risks, ensuring system\\nsafety and adherence to clinical, legal, and institutional standards across the task lifecycle.\\nIn operation, tasks from the Healthcare Application Layer are decomposed, allocated, and executed\\nacross agents, with outputs validated and integrated into coherent clinical responses. Two complementary\\nfeedback loops flow through the Agentic Collaboration Layer: (i) data feedback, in which user interactions,\\nclinical corrections, and contextual task information are routed back to enrich and update the Sustainable\\nMedical Data Ecosystem, and (ii) model feedback, where performance metrics, error patterns, and drift\\nsignals guide targeted model adaptation in the Adaptive Medical GenAI Layer. Together, these loops\\nenable continuous refinement of both data pipelines and model behaviors, ensuring SAGE-Health remains\\nresponsive to the evolving healthcare demands.\\n5.5\\nSAGE-Health in Action\\n5.5.1\\nWorkflow Illustration\\nTo illustrate the operational dynamics of SAGE-Health, we consider a representative clinical scenario:\\ngenerating a medical report from a chest X-ray image. Chest X-ray report generation is a core diagnostic\\ntask in radiology, requiring the transformation of complex medical images into structured, clinically\\nactionable reports that integrate visual findings with patient context. The end-to-end workflow of this\\nprocess is illustrated in Figure 5, which demonstrates how SAGE-Health coordinates data retrieval, model\\nselection, and expert agents to produce accurate and contextually grounded radiology reports, with the\\ninput chest X-ray image randomly selected from the MIMIC-CXR dataset [110].\\nIn SAGE-Health, this task begins in the Healthcare Application Layer, where a clinician submits\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n14\\nApplication\\nAccess &\\xa0\\nAggregate Data\\nGenerate\\nPreliminary Report\\nValidate & Refine\\nReport\\nSelect &\\xa0Adapt\\nModel\\nSubtasks\\nAgent Coordination Hub\\nData Retrieval\\nTask-oriented\\xa0\\nAgents\\nModel-oriented\\xa0\\nAgents\\nData-oriented\\xa0\\nAgents\\nGovernance\\xa0\\nAgents\\nExpert Agent Suite\\nModel Selection\\nVector\\xa0\\nSearch\\xa0Engine\\nData\\xa0Query\\nEngine\\nMedical Data\\nLakehouse\\nSimilar\\xa0Chest\\xa0\\nRadiography Cases\\nRetrieval-enhanced\\nPrompting\\nHealthGPT\\nRelevant\\xa0\\nClinical Context\\nRelevant\\nReport\\nTreatment\\nContext\\nLaboratory\\nResults\\n...\\nAs an expert radiologist, generate a\\nchest X-ray report with Findings and\\nImpression, informed by the retrieved\\nsimilar cases:\\nThe chest X-ray\\nshows normal-\\nappearing lungs. No\\nobvious\\nabnormalities are\\nidentified. The\\nheart size is\\nnormal.\\nPerformance\\nValidation\\nModel PEFT\\nHealthGPT\\nRelevant\\xa0\\nClinical Context\\nImpression\\nNo acute cardiopulmonary\\nprocess.\\nMedical\\nReport\\nGeneration\\xa0\\nRisk & Safety\\nOversight\\nTask Planner &\\nDecomposer\\nPolicy\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nEnforcement\\xa0\\xa0\\nImpression\\nNormal chest X-ray.\\nFindings\\nThe lungs are clear\\nwithout focal\\nconsolidation. No pleural\\neffusion or pneumothorax\\nis present. The cardiac\\nand mediastinal contours\\nare within normal limits.\\nAortic knob calcification\\nis noted.\\nFindings\\nData Feedback\\nCoordination\\nFigure 5\\nWorkflow of SAGE-Health for Radiology Report Generation from Chest X-rays.\\na chest X-ray through the Medical Report Generation & Documentation interface. The request is then\\nrelayed to the Agentic Collaboration Layer, where the Task Planner & Decomposer leverage the task-\\noriented agents in the Expert Agent Suite to interpret the clinical objective and formulate a multi-stage\\nplan: (1) accessing and aggregating the patient-specific context, (2) selecting and adapting an optimal\\nvision-language model for chest radiography, (3) generating a preliminary report, and (4) validating and\\nfinalizing the report.\\nThe Agent Coordination Hub subsequently orchestrates the execution of these\\nsubtasks by delegating them to the appropriate Expert Agents within the suite, ensuring that model-\\noriented, data-oriented, and governance functions are seamlessly integrated throughout the process.\\nSpecifically, the Data-oriented Agents access the Sustainable Medical Data Ecosystem, leverag-\\ning Vector Search Engine HAKES [111] to retrieve semantically similar chest radiography cases from\\ncurated archives. The retrieved data are then routed to the Model-oriented Agents, which invoke the\\nAdaptive Medical GenAI Layer to select an appropriate vision–language foundation model from the\\nFoundation Model Zoo, such as HealthGPT [10]. After an appropriate model is selected, targeted adap-\\ntation strategies in the model adaptation hub are realized through RAG, a retrieval-enhanced prompting\\nmethod that incorporates retrieved chest X-ray exemplars into the inference stage to guide draft report\\ngeneration. Following this step, the Model-oriented Agents conduct performance validation, which may\\ntrigger targeted updates of the model. To enable such updates, the Data-oriented Agents query the\\nSustainable Medical Data Ecosystem via the Data Query Engine to extract relevant clinical context\\n(e.g., longitudinal imaging records, prior radiology reports, and pertinent EHR segments). Leveraging\\nthis context, targeted adaptation strategies such as PEFT are employed to optimize model performance\\nfor the present case. The updated model is then reintegrated into the inference workflow to generate a\\nrefined medical report.\\nThe Privacy-Preserving Intelligence subsystem safeguards patient confidentiality during both model\\nadaptation and inference in chest X-ray report generation. For fine-tuning, raw chest X-rays and clinical\\nhistories remain within their originating institutions, where Model-oriented Agents conduct local feature\\nextraction and parameter updates inside secure execution environments.\\nOnly anonymized represen-\\ntations and encrypted weight deltas are transmitted for secure aggregation across sites, enabling model\\nrefinement without exposing identifiable imaging or EHR data. During the inference process, the patient’s\\nX-ray is transformed into privacy-preserving embeddings on-site, ensuring secure model improvement and\\nreport generation. Throughout this process, the Governance Agents operate continuously across the task\\nlifecycle: enforcing policy compliance during task planning, auditing privacy and regulatory adherence\\nduring data retrieval and model adaptation, and performing risk and safety oversight before the report\\nis released.\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n15\\nFindings\\nModerate left and small right pleural effusions persist. Heart size and mediastinal contours are unchanged. Lung \\nvolumes are slightly lower.\\nImpression\\nModerate left and small right pleural effusions.\\nQuery Image\\nFindings\\nHeart size appears at least mild to moderately enlarged, though difficult to assess given the presence of moderate size \\nbilateral pleural effusions, left greater than right. The size of these effusions appears increased compared to the \\nprevious exam. … most likely bibasilar compressive atelectasis.\\nImpression\\nModerate size bilateral pleural effusions, left greater than right, and increased from the prior study. Compressive \\nbibasilar atelectasis. Mild pulmonary vascular congestion.\\nRank-1 Retrieval \\nScore=0.9893\\nFindings\\n… interval improvement in mild pulmonary edema. A moderate right pleural effusion is stable and a small left pleural \\neffusion has also decreased in size. Aside from persistent bibasilar atelectasis, the lungs are clear.\\nImpression\\n… improvement in the mild pulmonary edema and decrease in the small left pleural effusion. Moderate right pleural \\neffusion and bibasilar atelectasis are stable.\\nRank-2 Retrieval \\nScore=0.9888\\nFindings\\n… Lung volumes are low. … Linear opacity within the right lung base is worse … compatible with progressive \\natelectasis. Minimal atelectasis is also noted in the left lung base. There may be a small right pleural effusion.\\nImpression\\n… Worsening atelectasis in the right lung base.\\nRank-2 Retrieval \\nScore=0.9875\\nFigure 6\\nRetrieval case study illustrating query image and top retrieved samples, implemented with HAKES [111] using BioViL-\\nT [7] as the vision encoder.\\nFindings\\nModerate left and small right pleural effusions persist. Heart size and \\nmediastinal contours are unchanged. Lung volumes are slightly lower.\\nImpression\\nModerate left and small right pleural effusions.\\nQuery Image\\nFindings\\nThe chest X-ray shows bilateral patchy opacities in the lung fields, more prominent in the \\nlower lung zones. There is a right-sided pleural effusion. The heart size is within normal limits, \\nand the mediastinum is not widened. The bony structures, including the ribs and clavicles, \\nappear intact.\\nImpression\\nThe chest X-ray reveals bilateral patchy opacities consistent with pneumonia or atelectasis, \\nalong with a right-sided pleural effusion. \\nFindings\\nThe chest X-ray shows a moderate-sized right pleural effusion, with a smaller left pleural \\neffusion. There is bibasilar atelectasis present, and the lungs appear clear otherwise. The hilar \\nand cardiomediastinal contours are normal, and there is no evidence of pneumothorax.\\nImpression\\nThe chest X-ray demonstrates a moderate right pleural effusion, a smaller left pleural effusion, \\nand bibasilar atelectasis. \\nGround-truth\\nw/o retrieval\\nw/ retrieval\\nFindings\\nThe heart appears mildly enlarged. There is no focal consolidation, \\neffusion, or pneumothorax. The cardiomediastinal silhouette is \\nnormal. Imaged osseous structures are intact. \\nImpression\\nMild cardiomegaly, otherwise unremarkable.\\nQuery Image\\nFindings\\nThe chest X-ray shows bilateral patchy opacities in the lung fields, more prominent in the \\nlower lung zones. The heart size appears normal, and the mediastinal structures are within \\nnormal limits. There is no evidence of pleural effusion or pneumothorax.\\nImpression\\nThe chest X-ray demonstrates bilateral patchy opacities, which may suggest an infectious or \\ninflammatory process such as pneumonia.\\nFindings\\nThe cardiac silhouette is mildly enlarged. The lung fields are clear, with no evidence of \\nconsolidation, pleural effusion, or pneumothorax. The bony structures, including the ribs and \\nspine, appear intact.\\nImpression\\nMild cardiomegaly. No acute cardiopulmonary process.\\nGround-truth\\nw/o retrieval\\nw/ retrieval\\nFigure 7\\nCase study of chest X-ray report generation with and without retrieval-augmented generation, using HealthGPT [10]\\nas report generation backbone and incorporating top-3 retrieved cases.\\nThis workflow is reinforced by two complementary feedback loops. The model feedback loop is acti-\\nvated through task-level performance indicators, such as diagnostic accuracy for rare thoracic findings,\\nand enables targeted model updates in the Adaptive Medical GenAI Layer. In parallel, a data feedback\\nloop operates downstream, where clinicians’ edits and diagnostic confirmations flow from the Health-\\ncare Application Layer through the Agentic Collaboration Layer’s Data-oriented Agents, which enrich\\nthem with task-specific context, and reintegrate the curated information into the Sustainable Medical\\nData Ecosystem to enhance future retrieval and semantic curation. Through this multi-layer collabo-\\nration, SAGE-Health delivers high-quality, contextually grounded chest X-ray reports that are accurate,\\ncompliant, and continuously improving.\\n5.5.2\\nCase Studies on Functional Capabilities of SAGE-Health\\nTo further validate the practical value of SAGE-Health, we present case studies in chest X-ray report\\ngeneration. Through functional demonstrations of retrieval-augmented generation and data–model co-\\nevolution via adaptive feedback, we illustrate how SAGE-Health transforms data into actionable evidence\\nand supports continuous refinement of generative outputs.\\nIn radiology, the ability to ground generative reports in prior clinical evidence is essential for both\\naccuracy and trust. Within SAGE-Health, retrieval fulfills this role by surfacing the most relevant chest\\nX-ray–report pairs from the MIMIC-CXR archive, returning a panel of top-ranked images and reports\\nthat highlight overlapping findings such as “pleural effusion” (Figure 6) and provide concrete precedents\\nfor downstream generation. When integrated into generation, this evidence directly reshapes the output.\\nAs illustrated in Figure 7, baseline reports without retrieval tend to rely on generic or partially incomplete\\nGenerative AI for Healthcare: Fundamentals, Challenges, and Perspectives\\n16\\nQuery Image\\nFindings\\nThe chest X-ray shows patchy opacities in the right lower lung zone. These are described as \\nconcerning for an infectious process, most consistent with pneumonia. The remaining lung \\nfields appear clear, and there is no large pleural effusion or pneumothorax. The \\ncardiomediastinal silhouette is within normal limits.\\nImpression\\nThe chest X-ray demonstrates a right lower lobe opacity, most consistent with pneumonia.\\nFindings\\nThe chest X-ray shows low lung volumes with linear opacity in the right lower lobe, compatible \\nwith atelectasis. Mild bibasilar crowding is present, but no consolidation, pleural effusion, or \\npneumothorax is identified. The cardiomediastinal silhouette remains stable.\\nImpression\\nThe chest X-ray demonstrates right lower lobe atelectasis; no acute cardiopulmonary \\nabnormality is seen.\\nFindings\\nPatchy opacities are seen in the right lower lobe, concerning for pneumonia.\\nImpression\\nRight lower lobe opacity, suspicious for pneumonia.\\nFindings\\nRight lower lobe atelectasis is present, stable compared to prior study; no features suggesting \\nacute pneumonia.\\nImpression\\nRight lower lobe atelectasis.\\nRetrieved Images\\nClinician Feedback\\nOriginal: “Patchy opacities in the right lower lobe, consistent with pneumonia.\"\\nCorrection: \"Right lower lobe atelectasis, no evidence of acute pneumonia.\"\\nRationale: Review of prior imaging and clinical history confirms chronic right lower lobe atelectasis.\\nRelabel & re-index case, propagate correction to similar cases\\nBefore feedback\\nAfter feedback\\nFigure 8\\nCase study on data–model co-evolution via adaptive feedback.\\ndescriptions, whereas retrieval-augmented outputs incorporate precise, clinically meaningful phrases that\\nare directly traceable to retrieved examples.\\nSAGE-Health further advances functionality by operationalizing clinician feedback as a driver of con-\\ntinuous data–model co-evolution. A common clinical scenario arises when an opacity initially read as\\nsuspicious for pneumonia is later corrected to atelectasis based on prior imaging and clinical history. In\\nSAGE-Health, such feedback is not treated as an isolated correction but is systematically integrated into\\nthe ecosystem: the case is relabelled, re-indexed, and linked to similar cases for further verification and\\ncorrection. By reshaping the retrieval context, the system ensures that when the same image is repro-\\ncessed, the report moves from an uncertain pneumonia diagnosis to a precise description of atelectasis,\\nas illustrated in Figure 8. This demonstration highlights how adaptive feedback closes the loop between\\nclinicians, data infrastructure, and generative models, enabling practical data–model co-evolution that\\nenhances factual accuracy and clinical trust without requiring full retraining.\\n6\\nConclusions\\nGenerative Artificial Intelligence (GenAI) holds immense promise for healthcare, but its clinical impact\\nwill remain limited without rethinking the data foundations that sustain it. In this paper, we propose a\\ndata-centric paradigm that positions medical data ecosystems as the foundational substrate for generative\\nhealthcare systems. We present SAGE-Health, a sustainable, adaptive, and generative framework that\\ntackles three interlocking challenges: data fragmentation, data life cycle governance, and data–model\\nco-evolution, enabling GenAI to progress from proof-of-concept to reliable, scalable deployment.\\nBy\\nexploiting data-centric GenAI systems as the engine of progress, we envision a future in which GenAI\\nbecomes a trustworthy tool in delivering truly transformative healthcare.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765e20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(documents[0])\n",
    "\n",
    "vectorstore = FAISS.from_documents(splitted_documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72836726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='921f1ebe-2141-42ee-bf03-4612fd64b719', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='grating heterogeneous biomedical data, generating context-aware outputs, and adapting to diverse clinical\\nscenarios. In practice, such strengths have driven progress across three pivotal domains: (i) Disease Di-\\nagnosis and Decision Support, where multimodal generative models unify imaging, physiological signals,\\nand clinical narratives to support decision-making; (ii) Medical Report Generation and Documentation,\\nwhere language models automate the synthesis of structured and unstructured records; and (iii) Drug\\nDiscovery and Biomedical Research, where molecular and protein language models accelerate hypothesis\\ngeneration and compound design. The following subsections review representative efforts in these areas,\\nanalyzing how generative and multimodal foundation models have been developed for modern healthcare\\napplications.\\n3.1\\nDisease Diagnosis and Decision Support\\nDisease diagnosis presents a uniquely complex challenge in healthcare, requiring the integration of diverse'),\n",
       " Document(id='61362590-9132-4306-9947-e5029fa6efb6', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='radiograph diagnostic benchmarks.\\nBeyond radiology, VLMs are also being tailored to other clinical\\nspecialties. For instance, OphGLM [70] extends this paradigm to ophthalmology, aligning ocular imag-\\ning with domain-specific knowledge for improved diagnostic reasoning in eye disease assessment, while\\nEchoCLIP [71] is an LVLM for echocardiography trained on over one million cardiac ultrasound videos\\nand expert interpretations, enabling accurate cardiac function assessment and patient-level reasoning\\nacross studies.\\nAgent-Based Diagnostic Assistants. Agent-based frameworks have recently emerged as a promising\\nparadigm for disease diagnosis and clinical decision support, addressing the limitations of standalone\\nLLMs by enabling multi-agent collaboration [72]. In these methods, agents are assigned specialized roles,\\nsuch as intent recognition, diagnostic reasoning, and treatment planning, to create a context-aware and\\ninteractive healthcare delivery process.'),\n",
       " Document(id='a5f38503-f035-4f8c-a263-6741a05c796b', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='questions about biomedical images, supporting flexible diagnostic exploration.\\nIn addition, specialized VLMs have been developed for targeted diagnostic applications, with ra-\\ndiograph interpretation emerging as a major focus [9, 67–69].\\nKnowledge-enhanced Auto Diagnosis\\n(KAD) [69] integrates structured medical domain knowledge into vision–language pretraining, align-\\ning chest X-ray images with knowledge-grounded radiology report representations.\\nBy incorporating\\nontology-based concept encoding and disease-query mechanisms, KAD advances automated diagnosis for\\nchest X-ray images by leveraging domain knowledge. CARZero [9] replaces traditional cosine similarity-\\nbased alignment with cross-attention mechanisms and incorporates an LLM-driven prompt standard-\\nization strategy for zero-shot radiology classification, achieving state-of-the-art performance on chest\\nradiograph diagnostic benchmarks.\\nBeyond radiology, VLMs are also being tailored to other clinical'),\n",
       " Document(id='b14025ad-e74c-4a1e-bda9-7efff9be3676', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='are being developed to address narrower clinical domains and patient populations. For instance, Pedi-\\natricsGPT [64] is a Chinese pediatric assistant LLM fine-tuned on pediatric guidelines and case-based\\ndialogues, providing tailored responses for pediatric care in Chinese clinical contexts. These develop-\\nments illustrate a continuum from broad-coverage generalists to focused domain experts, highlighting\\nhow strategic pretraining and fine-tuning align LLMs with the nuanced demands of clinical practice.\\nLVLMs for Diagnostic Reasoning.\\nThrough cross-modal pretraining and alignment, VLMs are\\ncapable of integrating biomedical imaging with domain-specific textual information, thereby providing\\ninterpretable, context-aware diagnostic insights.\\nFor example, LLaVA-Med [66] offers a cost-efficient\\nframework for training vision-language conversational assistants capable of answering open-ended research\\nquestions about biomedical images, supporting flexible diagnostic exploration.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.as_retriever().invoke(\"Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2066b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorstore\n",
    "vectorstore.save_local(\"vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c797f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "vectorstore = FAISS.load_local(\"vector_index\", embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dc0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='921f1ebe-2141-42ee-bf03-4612fd64b719', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='grating heterogeneous biomedical data, generating context-aware outputs, and adapting to diverse clinical\\nscenarios. In practice, such strengths have driven progress across three pivotal domains: (i) Disease Di-\\nagnosis and Decision Support, where multimodal generative models unify imaging, physiological signals,\\nand clinical narratives to support decision-making; (ii) Medical Report Generation and Documentation,\\nwhere language models automate the synthesis of structured and unstructured records; and (iii) Drug\\nDiscovery and Biomedical Research, where molecular and protein language models accelerate hypothesis\\ngeneration and compound design. The following subsections review representative efforts in these areas,\\nanalyzing how generative and multimodal foundation models have been developed for modern healthcare\\napplications.\\n3.1\\nDisease Diagnosis and Decision Support\\nDisease diagnosis presents a uniquely complex challenge in healthcare, requiring the integration of diverse'),\n",
       " Document(id='61362590-9132-4306-9947-e5029fa6efb6', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='radiograph diagnostic benchmarks.\\nBeyond radiology, VLMs are also being tailored to other clinical\\nspecialties. For instance, OphGLM [70] extends this paradigm to ophthalmology, aligning ocular imag-\\ning with domain-specific knowledge for improved diagnostic reasoning in eye disease assessment, while\\nEchoCLIP [71] is an LVLM for echocardiography trained on over one million cardiac ultrasound videos\\nand expert interpretations, enabling accurate cardiac function assessment and patient-level reasoning\\nacross studies.\\nAgent-Based Diagnostic Assistants. Agent-based frameworks have recently emerged as a promising\\nparadigm for disease diagnosis and clinical decision support, addressing the limitations of standalone\\nLLMs by enabling multi-agent collaboration [72]. In these methods, agents are assigned specialized roles,\\nsuch as intent recognition, diagnostic reasoning, and treatment planning, to create a context-aware and\\ninteractive healthcare delivery process.'),\n",
       " Document(id='a5f38503-f035-4f8c-a263-6741a05c796b', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='questions about biomedical images, supporting flexible diagnostic exploration.\\nIn addition, specialized VLMs have been developed for targeted diagnostic applications, with ra-\\ndiograph interpretation emerging as a major focus [9, 67–69].\\nKnowledge-enhanced Auto Diagnosis\\n(KAD) [69] integrates structured medical domain knowledge into vision–language pretraining, align-\\ning chest X-ray images with knowledge-grounded radiology report representations.\\nBy incorporating\\nontology-based concept encoding and disease-query mechanisms, KAD advances automated diagnosis for\\nchest X-ray images by leveraging domain knowledge. CARZero [9] replaces traditional cosine similarity-\\nbased alignment with cross-attention mechanisms and incorporates an LLM-driven prompt standard-\\nization strategy for zero-shot radiology classification, achieving state-of-the-art performance on chest\\nradiograph diagnostic benchmarks.\\nBeyond radiology, VLMs are also being tailored to other clinical'),\n",
       " Document(id='b14025ad-e74c-4a1e-bda9-7efff9be3676', metadata={'Published': '2025-10-28', 'Title': 'Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives', 'Authors': 'Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi', 'Summary': 'Generative Artificial Intelligence (GenAI) is taking the world by storm. It\\npromises transformative opportunities for advancing and disrupting existing\\npractices, including healthcare. From large language models (LLMs) for clinical\\nnote synthesis and conversational assistance to multimodal systems that\\nintegrate medical imaging, electronic health records, and genomic data for\\ndecision support, GenAI is transforming the practice of medicine and the\\ndelivery of healthcare, such as diagnosis and personalized treatments, with\\ngreat potential in reducing the cognitive burden on clinicians, thereby\\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\\nrequires an in-depth understanding of healthcare tasks and what can and cannot\\nbe achieved. In this paper, we propose a data-centric paradigm in the design\\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\\ndata life cycle by making the medical data ecosystem as the foundational\\nsubstrate for generative healthcare systems. This ecosystem is designed to\\nsustainably support the integration, representation, and retrieval of diverse\\nmedical data and knowledge. With effective and efficient data processing\\npipelines, such as semantic vector search and contextual querying, it enables\\nGenAI-powered operations for upstream model components and downstream clinical\\napplications. Ultimately, it not only supplies foundation models with\\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\\nfine-tuning, but also serves as a knowledge retrieval backend to support\\ntask-specific inference via the agentic layer. The ecosystem enables the\\ndeployment of GenAI for high-quality and effective healthcare delivery.'}, page_content='are being developed to address narrower clinical domains and patient populations. For instance, Pedi-\\natricsGPT [64] is a Chinese pediatric assistant LLM fine-tuned on pediatric guidelines and case-based\\ndialogues, providing tailored responses for pediatric care in Chinese clinical contexts. These develop-\\nments illustrate a continuum from broad-coverage generalists to focused domain experts, highlighting\\nhow strategic pretraining and fine-tuning align LLMs with the nuanced demands of clinical practice.\\nLVLMs for Diagnostic Reasoning.\\nThrough cross-modal pretraining and alignment, VLMs are\\ncapable of integrating biomedical imaging with domain-specific textual information, thereby providing\\ninterpretable, context-aware diagnostic insights.\\nFor example, LLaVA-Med [66] offers a cost-efficient\\nframework for training vision-language conversational assistants capable of answering open-ended research\\nquestions about biomedical images, supporting flexible diagnostic exploration.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.as_retriever().invoke(\"Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb6373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': vectorstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48beab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in chain.stream(message):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dd4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiro13/Desktop/LLMs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/tw/s773v1kd5vd7s09s1y4qjktw0000gn/T/ipykernel_5021/1379051613.py:8: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
      "/Users/chiro13/Desktop/LLMs/.venv/lib/python3.11/site-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://86d228a841a82b4ead.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://86d228a841a82b4ead.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://86d228a841a82b4ead.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    \" I have access to the following Paper: Attention Is All You Need\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d816b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is an encoder made of?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
