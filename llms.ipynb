{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e9fbb8",
   "metadata": {},
   "source": [
    "# Como usar LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5659bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360024d",
   "metadata": {},
   "source": [
    "[NVIDIA api key](https://build.nvidia.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d720eb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dotenv loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Dotenv loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafcb0d",
   "metadata": {},
   "source": [
    "## A pedal\n",
    "\n",
    "Usando la libreria `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835d15b7-eefe-4232-8536-5e72b2476a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      " - 01-ai/yi-large\n",
      " - abacusai/dracarys-llama-3.1-70b-instruct\n",
      " - adept/fuyu-8b\n",
      " - ai21labs/jamba-1.5-large-instruct\n",
      " - ai21labs/jamba-1.5-mini-instruct\n",
      " - aisingapore/sea-lion-7b-instruct\n",
      " - baai/bge-m3\n",
      " - baichuan-inc/baichuan2-13b-chat\n",
      " - bigcode/starcoder2-15b\n",
      " - bigcode/starcoder2-7b\n",
      " - bytedance/seed-oss-36b-instruct\n",
      " - databricks/dbrx-instruct\n",
      " - deepseek-ai/deepseek-coder-6.7b-instruct\n",
      " - deepseek-ai/deepseek-r1\n",
      " - deepseek-ai/deepseek-r1-0528\n",
      " - deepseek-ai/deepseek-r1-distill-llama-8b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-14b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-32b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-7b\n",
      " - deepseek-ai/deepseek-v3.1\n",
      " - deepseek-ai/deepseek-v3.1-terminus\n",
      " - google/codegemma-1.1-7b\n",
      " - google/codegemma-7b\n",
      " - google/deplot\n",
      " - google/gemma-2-27b-it\n",
      " - google/gemma-2-2b-it\n",
      " - google/gemma-2-9b-it\n",
      " - google/gemma-2b\n",
      " - google/gemma-3-12b-it\n",
      " - google/gemma-3-1b-it\n",
      " - google/gemma-3-27b-it\n",
      " - google/gemma-3-4b-it\n",
      " - google/gemma-3n-e2b-it\n",
      " - google/gemma-3n-e4b-it\n",
      " - google/gemma-7b\n",
      " - google/paligemma\n",
      " - google/recurrentgemma-2b\n",
      " - google/shieldgemma-9b\n",
      " - gotocompany/gemma-2-9b-cpt-sahabatai-instruct\n",
      " - ibm/granite-3.0-3b-a800m-instruct\n",
      " - ibm/granite-3.0-8b-instruct\n",
      " - ibm/granite-3.3-8b-instruct\n",
      " - ibm/granite-34b-code-instruct\n",
      " - ibm/granite-8b-code-instruct\n",
      " - ibm/granite-guardian-3.0-8b\n",
      " - igenius/colosseum_355b_instruct_16k\n",
      " - igenius/italia_10b_instruct_16k\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1\n",
      " - marin/marin-8b-instruct\n",
      " - mediatek/breeze-7b-instruct\n",
      " - meta/codellama-70b\n",
      " - meta/llama-3.1-405b-instruct\n",
      " - meta/llama-3.1-70b-instruct\n",
      " - meta/llama-3.1-8b-instruct\n",
      " - meta/llama-3.2-11b-vision-instruct\n",
      " - meta/llama-3.2-1b-instruct\n",
      " - meta/llama-3.2-3b-instruct\n",
      " - meta/llama-3.2-90b-vision-instruct\n",
      " - meta/llama-3.3-70b-instruct\n",
      " - meta/llama-4-maverick-17b-128e-instruct\n",
      " - meta/llama-4-scout-17b-16e-instruct\n",
      " - meta/llama-guard-4-12b\n",
      " - meta/llama2-70b\n",
      " - meta/llama3-70b-instruct\n",
      " - meta/llama3-8b-instruct\n",
      " - microsoft/kosmos-2\n",
      " - microsoft/phi-3-medium-128k-instruct\n",
      " - microsoft/phi-3-medium-4k-instruct\n",
      " - microsoft/phi-3-mini-128k-instruct\n",
      " - microsoft/phi-3-mini-4k-instruct\n",
      " - microsoft/phi-3-small-128k-instruct\n",
      " - microsoft/phi-3-small-8k-instruct\n",
      " - microsoft/phi-3-vision-128k-instruct\n",
      " - microsoft/phi-3.5-mini-instruct\n",
      " - microsoft/phi-3.5-moe-instruct\n",
      " - microsoft/phi-3.5-vision-instruct\n",
      " - microsoft/phi-4-mini-flash-reasoning\n",
      " - microsoft/phi-4-mini-instruct\n",
      " - microsoft/phi-4-multimodal-instruct\n",
      " - minimaxai/minimax-m2\n",
      " - mistralai/codestral-22b-instruct-v0.1\n",
      " - mistralai/magistral-small-2506\n",
      " - mistralai/mamba-codestral-7b-v0.1\n",
      " - mistralai/mathstral-7b-v0.1\n",
      " - mistralai/mistral-7b-instruct-v0.2\n",
      " - mistralai/mistral-7b-instruct-v0.3\n",
      " - mistralai/mistral-large\n",
      " - mistralai/mistral-large-2-instruct\n",
      " - mistralai/mistral-medium-3-instruct\n",
      " - mistralai/mistral-nemotron\n",
      " - mistralai/mistral-small-24b-instruct\n",
      " - mistralai/mistral-small-3.1-24b-instruct-2503\n",
      " - mistralai/mixtral-8x22b-instruct-v0.1\n",
      " - mistralai/mixtral-8x22b-v0.1\n",
      " - mistralai/mixtral-8x7b-instruct-v0.1\n",
      " - moonshotai/kimi-k2-instruct\n",
      " - moonshotai/kimi-k2-instruct-0905\n",
      " - nv-mistralai/mistral-nemo-12b-instruct\n",
      " - nvidia/embed-qa-4\n",
      " - nvidia/llama-3.1-nemoguard-8b-content-safety\n",
      " - nvidia/llama-3.1-nemoguard-8b-topic-control\n",
      " - nvidia/llama-3.1-nemotron-51b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-reward\n",
      " - nvidia/llama-3.1-nemotron-nano-4b-v1.1\n",
      " - nvidia/llama-3.1-nemotron-nano-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-nano-vl-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-safety-guard-8b-v3\n",
      " - nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
      " - nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\n",
      " - nvidia/llama-3.2-nemoretriever-300m-embed-v1\n",
      " - nvidia/llama-3.2-nemoretriever-300m-embed-v2\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v1\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v2\n",
      " - nvidia/llama-3.3-nemotron-super-49b-v1\n",
      " - nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      " - nvidia/llama3-chatqa-1.5-70b\n",
      " - nvidia/llama3-chatqa-1.5-8b\n",
      " - nvidia/mistral-nemo-minitron-8b-8k-instruct\n",
      " - nvidia/mistral-nemo-minitron-8b-base\n",
      " - nvidia/nemoretriever-parse\n",
      " - nvidia/nemotron-4-340b-instruct\n",
      " - nvidia/nemotron-4-340b-reward\n",
      " - nvidia/nemotron-4-mini-hindi-4b-instruct\n",
      " - nvidia/nemotron-mini-4b-instruct\n",
      " - nvidia/nemotron-nano-12b-v2-vl\n",
      " - nvidia/nemotron-parse\n",
      " - nvidia/neva-22b\n",
      " - nvidia/nv-embed-v1\n",
      " - nvidia/nv-embedcode-7b-v1\n",
      " - nvidia/nv-embedqa-e5-v5\n",
      " - nvidia/nv-embedqa-mistral-7b-v2\n",
      " - nvidia/nvclip\n",
      " - nvidia/nvidia-nemotron-nano-9b-v2\n",
      " - nvidia/riva-translate-4b-instruct\n",
      " - nvidia/usdcode-llama-3.1-70b-instruct\n",
      " - nvidia/vila\n",
      " - openai/gpt-oss-120b\n",
      " - openai/gpt-oss-120b\n",
      " - openai/gpt-oss-20b\n",
      " - openai/gpt-oss-20b\n",
      " - opengpt-x/teuken-7b-instruct-commercial-v0.4\n",
      " - qwen/qwen2-7b-instruct\n",
      " - qwen/qwen2.5-7b-instruct\n",
      " - qwen/qwen2.5-coder-32b-instruct\n",
      " - qwen/qwen2.5-coder-7b-instruct\n",
      " - qwen/qwen3-235b-a22b\n",
      " - qwen/qwen3-coder-480b-a35b-instruct\n",
      " - qwen/qwen3-next-80b-a3b-instruct\n",
      " - qwen/qwen3-next-80b-a3b-thinking\n",
      " - qwen/qwq-32b\n",
      " - rakuten/rakutenai-7b-chat\n",
      " - rakuten/rakutenai-7b-instruct\n",
      " - sarvamai/sarvam-m\n",
      " - snowflake/arctic-embed-l\n",
      " - speakleash/bielik-11b-v2.3-instruct\n",
      " - speakleash/bielik-11b-v2.6-instruct\n",
      " - stockmark/stockmark-2-100b-instruct\n",
      " - thudm/chatglm3-6b\n",
      " - tiiuae/falcon3-7b-instruct\n",
      " - tokyotech-llm/llama-3-swallow-70b-instruct-v0.1\n",
      " - upstage/solar-10.7b-instruct\n",
      " - utter-project/eurollm-9b-instruct\n",
      " - writer/palmyra-creative-122b\n",
      " - writer/palmyra-fin-70b-32k\n",
      " - writer/palmyra-med-70b\n",
      " - writer/palmyra-med-70b-32k\n",
      " - yentinglin/llama-3-taiwan-70b-instruct\n",
      " - zyphra/zamba2-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "invoke_url = \"https://integrate.api.nvidia.com/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "print(\"Available Models:\")\n",
    "response = requests.get(invoke_url, headers=headers, stream=False)\n",
    "for model_entry in response.json().get(\"data\", []):\n",
    "    print(\" -\", model_entry.get(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed43fee2-0434-4709-b61f-2df26160bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "## If you wanted to use OpenAI, it's very similar\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY: \")\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "## Meta communication-level info about who you are, what you want, etc.\n",
    "headers = {\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "## Arguments to your server function\n",
    "payload = {\n",
    "    \"model\": \"meta/llama-3.3-70b-instruct\",\n",
    "    \"messages\": [{\"role\":\"user\", \"content\":\"Tell me hello in Japanese\"}],\n",
    "    \"temperature\": 0.5,   \n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True                \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56d6937-449d-465d-a257-a6e88ab7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konnichiwa! (Hello!)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "## Use requests.post to send the header (streaming meta-info) the payload to the endpoint\n",
    "## Make sure streaming is enabled, and expect the response to have an iter_lines response.\n",
    "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "## If your response is an error message, this will raise an exception in Python\n",
    "try: \n",
    "    response.raise_for_status()  ## If not 200 or similar, this will raise an exception\n",
    "except Exception as e:\n",
    "    # print(response.json())\n",
    "    print(response.json())\n",
    "    raise e\n",
    "\n",
    "## Custom utility to make live a bit easier\n",
    "def get_stream_token(entry: bytes):\n",
    "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
    "    if not entry: return \"\"\n",
    "    entry = entry.decode('utf-8')\n",
    "    if entry.startswith('data: '):\n",
    "        try: entry = json.loads(entry[5:])\n",
    "        except ValueError: return \"\"\n",
    "    return entry.get('choices', [{}])[0].get('delta', {}).get('content') or \"\"\n",
    "\n",
    "## If the post request is honored, you should be able to iterate over \n",
    "for line in response.iter_lines():\n",
    "    \n",
    "    ## Without Processing: data: {\"id\":\"...\", ... \"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}...}...\n",
    "    # if line: print(line.decode(\"utf-8\"))\n",
    "\n",
    "    ## With Processing: An actual stream of tokens printed one-after-the-other as they come in\n",
    "    print(get_stream_token(line), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713746",
   "metadata": {},
   "source": [
    "## Usando clientes como OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f324d88-94e0-480a-9ad6-2ab1aed3eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Konnichiwa) - This is a formal way of saying \"hello\" in Japanese. \n",
      "\n",
      "There are other ways to say hello in Japanese depending on the time of day:\n",
      "- (Ohayou) - Good morning\n",
      "- (Konbanwa) - Good evening\n",
      "\n",
      "However, (Konnichiwa) is a general greeting that can be used at any time of day."
     ]
    }
   ],
   "source": [
    "## Using General OpenAI Client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me hello in Japanese\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "## Streaming with Generator: Results come out as they're generated\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f17380f-eb8f-4cf8-9589-4d8300cb611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in Japanese), which is pronounced as \"Konnichiwa\". However, there are other ways to say hello in Japanese depending on the time of day:\n",
      "\n",
      "* (Ohayou) - Good morning\n",
      "* (Konnichiwa) - Good day (or hello)\n",
      "* (Konbanwa) - Good evening\n",
      "\n",
      "So, I'll say: (Konnichiwa)!"
     ]
    }
   ],
   "source": [
    "# Non-Streaming: Results come from server when they're all ready\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me hello in Japanese\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc73dc",
   "metadata": {},
   "source": [
    "## Usando langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36f5990-8f3e-4359-b46f-2deeaad6b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='(Konnichiwa)\\n\\nThis is a formal way of saying \"hello\" in Japanese. If you want to be more casual, you can say:\\n\\n(Konnichiwa) is used during the day, but if you want to greet someone in the evening, you can say:\\n\\n(Konbanwa)\\n\\nAnd if you want to greet someone in the morning, you can say:\\n\\n(Ohayou)\\n\\nLet me know if you have any other questions.', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': '(Konnichiwa)\\n\\nThis is a formal way of saying \"hello\" in Japanese. If you want to be more casual, you can say:\\n\\n(Konnichiwa) is used during the day, but if you want to greet someone in the evening, you can say:\\n\\n(Konbanwa)\\n\\nAnd if you want to greet someone in the morning, you can say:\\n\\n(Ohayou)\\n\\nLet me know if you have any other questions.', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None, 'token_usage': {'prompt_tokens': 40, 'total_tokens': 132, 'completion_tokens': 92, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.3-70b-instruct'}, id='lc_run--0c343d72-9b3c-4419-a7f9-eacedac7f307-0', usage_metadata={'input_tokens': 40, 'output_tokens': 92, 'total_tokens': 132}, role='assistant')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "#from langchain_openai import OpenAI\n",
    "\n",
    "## NVIDIA_API_KEY pulled from environment\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")\n",
    "\n",
    "llm.invoke(\"Tell me hello in Japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b1f8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konnichiwa! (Hello!)"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "print(chain.invoke(\"Tell me hello in Japanese\"), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cbfe72a-b69b-4fe1-8d21-bf45c1ba6789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://integrate.api.nvidia.com/v1/chat/completions',\n",
       " 'headers': {'Accept': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints',\n",
       "  'X-BILLING-SOURCE': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'user',\n",
       "    'content': 'Tell me hello in Japanese'}],\n",
       "  'model': 'meta/llama-3.3-70b-instruct',\n",
       "  'max_tokens': 1024,\n",
       "  'stream': False}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a82418e9-71fd-45ab-a558-9c17ea696864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-18750fa82054479d9e8789c32ec2ad0a',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1762111077,\n",
       " 'model': 'meta/llama-3.3-70b-instruct',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Konnichiwa! (Hello!)',\n",
       "    'refusal': None,\n",
       "    'annotations': None,\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': [],\n",
       "    'reasoning_content': None},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None,\n",
       "   'token_ids': None}],\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'prompt_tokens': 40,\n",
       "  'total_tokens': 49,\n",
       "  'completion_tokens': 9,\n",
       "  'prompt_tokens_details': None},\n",
       " 'prompt_logprobs': None,\n",
       " 'prompt_token_ids': None,\n",
       " 'kv_transfer_params': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm._client.last_response\n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f0ab9-0751-4fd5-a44b-16ae2c4c18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL: meta/llama-4-maverick-17b-128e-instruct\n",
      "I'm an AI assistant designed to provide helpful and informative responses to your questions and engage in conversation. I don't have a personal identity or emotions, but I'm here to assist you with any topics or tasks you'd like to discuss or accomplish!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-guard-4-12b\n",
      "safe\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama3-8b-instruct\n",
      "I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I can understand and respond to human input in a conversational manner, capable of generating human-like text based on the input given to me.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-11b-vision-instruct\n",
      "I am an artificial intelligence language model designed to provide information and answer questions on a wide range of topics. I don't have personal experiences or emotions, but I can assist with tasks, offer suggestions, and engage in conversation to the best of my abilities.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.3-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text, allowing me to have conversations and answer questions to the best of my knowledge. I don't have personal experiences or emotions, but I'm constantly learning and improving my responses based on the interactions I have with users like you.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama3-70b-instruct\n",
      "I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner, trained on a massive dataset of text to generate human-like responses. I don't have personal experiences, emotions, or physical existence, but I'm designed to assist and provide helpful information on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-4-scout-17b-16e-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I'm here to help users like you with information, answers, and tasks, and I can adapt to a wide range of topics and conversational styles!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.1-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and respond to human language, generating text based on the input I receive. I'm a large language model, I don't have a personal identity or consciousness, but I'm here to help answer your questions, provide information, and assist with tasks to the best of my abilities!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-1b-instruct\n",
      "I'm an artificial intelligence designed to assist and communicate with humans in a helpful and informative way. I don't have a personal identity or experiences, but I'm here to help answer your questions and provide insights on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama2-70b\n",
      "EXCEPTION: [404] Not Found\n",
      "Function '2fddadfb-7e76-4c8a-9b82-f7d3fab94471': Not found for account 'Ov4p4rPXRzfERG5L7lDsb7yR4WMpCeQraAUfZfRu2Dc'\n",
      "\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-3b-instruct\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model_list = ChatNVIDIA.get_available_models()\n",
    "\n",
    "for model_card in model_list:\n",
    "    model_name = model_card.id\n",
    "    ## If you want to, might be a good idea to not go through EVERY model\n",
    "    if not any([keyword in model_name for keyword in [\"meta/llama\"]]): continue\n",
    "    if \"405b\" in model_name: continue\n",
    "    if \"embed\" in model_name: continue\n",
    "    \n",
    "    llm = ChatNVIDIA(model=model_name)\n",
    "    print(f\"TRIAL: {model_name}\")\n",
    "    try: \n",
    "        for token in llm.stream(\"Tell me about yourself! 2 sentences.\", max_tokens=100):\n",
    "            print(token.content, end=\"\")\n",
    "    except Exception as e: \n",
    "        print(f\"EXCEPTION: {e}\")    ## If some models fail, feel free to use others\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Stopped manually\")  ## Feel free to hit square while running\n",
    "        break\n",
    "    print(\"\\n\\n\" + \"=\"*84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
